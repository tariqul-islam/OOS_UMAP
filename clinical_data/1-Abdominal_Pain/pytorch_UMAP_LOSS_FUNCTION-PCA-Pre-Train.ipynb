{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy import optimize\n",
    "\n",
    "from scipy.io import savemat\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "dataset = 3\n",
    "\n",
    "if dataset == 1:\n",
    "    expr = pd.read_csv('CAFs.txt', sep='\\t')\n",
    "    X_train = expr.values[:,0:(expr.shape[1]-1)]\n",
    "    X_train = np.log(X_train + 1)\n",
    "    n = X_train.shape[0]\n",
    "    print(\"\\nThis data set contains \" + str(n) + \" samples\")\n",
    "    y_train = np.array(expr.values[:,expr.shape[1]-1],dtype=np.int)\n",
    "    print(\"\\nDimensions of the  data set: \")\n",
    "    print(X_train.shape, y_train.shape)\n",
    "\n",
    "elif dataset == 2:\n",
    "    import mnist_reader\n",
    "\n",
    "    X_train, y_train = mnist_reader.load_mnist('data/mnist', kind='train')\n",
    "    X_test, y_test = mnist_reader.load_mnist('data/mnist', kind='t10k')\n",
    "    \n",
    "    n = 2000\n",
    "    #np.random.seed(6333)\n",
    "    #ids = np.random.choice(range(60000),n)\n",
    "    #X_train, y_train = X_train[ids], y_train[ids]\n",
    "    \n",
    "    n = X_train.shape[0]\n",
    "    \n",
    "    \n",
    "    X_train = X_train.astype(np.double)\n",
    "    X_train = X_train/(np.max(X_train))\n",
    "    print(X_train.dtype)\n",
    "    \n",
    "    classes = [\n",
    "        '0',\n",
    "        '1',\n",
    "        '2',\n",
    "        '3',\n",
    "        '4',\n",
    "        '5',\n",
    "        '6',\n",
    "        '7',\n",
    "        '8',\n",
    "        '9']\n",
    "    \n",
    "    print(y_train)\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    \n",
    "    \n",
    "elif dataset == 3:\n",
    "    import mnist_reader \n",
    "    \n",
    "    X_train, y_train = mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "    X_test, y_test = mnist_reader.load_mnist('data/fashion', kind='t10k')\n",
    "    \n",
    "    #n = 2000\n",
    "    #np.random.seed(6333)\n",
    "    #ids = np.random.choice(range(60000),n)\n",
    "    #X_train, y_train = X_train[ids], y_train[ids]\n",
    "    \n",
    "    \n",
    "    \n",
    "    X_train = X_train.astype(np.float32)\n",
    "    max_val = np.max(X_train)\n",
    "    X_train = X_train/max_val\n",
    "    \n",
    "    X_test = X_test/max_val\n",
    "    \n",
    "    n = X_train.shape[0]\n",
    "    \n",
    "    classes = [\n",
    "        'T-shirt/top',\n",
    "        'Trouser',\n",
    "        'Pullover',\n",
    "        'Dress',\n",
    "        'Coat',\n",
    "        'Sandal',\n",
    "        'Shirt',\n",
    "        'Sneaker',\n",
    "        'Bag',\n",
    "        'Ankle boot']\n",
    "    \n",
    "    print(X_train.shape, y_train.shape, X_train.dtype)\n",
    "    \n",
    "%matplotlib notebook\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors= 30\n",
    "MIN_DIST = 0.001\n",
    "\n",
    "import umap\n",
    "\n",
    "ump = umap.UMAP(n_neighbors=n_neighbors,\n",
    "        min_dist=MIN_DIST,\n",
    "        n_components=2,\n",
    "        random_state=100,\n",
    "        metric= 'euclidean')\n",
    "\n",
    "y_umap = ump.fit_transform(X_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(y_umap[:,0], y_umap[:,1], c=y_train, s=0.01, cmap='Spectral')\n",
    "\n",
    "cbar = plt.colorbar(boundaries=np.arange(11)-0.5)\n",
    "cbar.set_ticks(np.arange(10))\n",
    "cbar.set_ticklabels(classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(y_umap[:,0], y_umap[:,1], c=y_train, s=0.01, cmap='Spectral')\n",
    "\n",
    "cbar = plt.colorbar(boundaries=np.arange(11)-0.5)\n",
    "cbar.set_ticks(np.arange(10))\n",
    "cbar.set_ticklabels(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba import prange\n",
    "import random\n",
    "\n",
    "\n",
    "def correlation_distances_old(X):\n",
    "    n = X.shape[0]\n",
    "    xcorr = np.zeros((n,n),dtype=X.dtype)\n",
    "    for i in range(n):\n",
    "        for j in range(i,n):\n",
    "            corr = np.sum(X[i,:] * X[j,:])\n",
    "            xcorr[i,j] = corr\n",
    "            xcorr[j,i] = corr\n",
    "    \n",
    "    return xcorr\n",
    "'''\n",
    "@numba.jit(nopython=True, parallel=True)\n",
    "def correlation_distances(X):\n",
    "    n = X.shape[0]\n",
    "    xcorr = np.zeros((n,n),dtype=X.dtype)\n",
    "    for i in prange(n):\n",
    "        for j in prange(i,n):\n",
    "            corr = np.sum(X[i,:] * X[j,:])\n",
    "            xcorr[i,j] = corr\n",
    "            xcorr[j,i] = corr\n",
    "    \n",
    "    return xcorr\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = euclidean_distances(X_train, squared = False)\n",
    "\n",
    "#print(dist[0:4, 0:4])\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_idx = np.argsort(dist,axis=1)\n",
    "\n",
    "sort_idx = sort_idx[:,1:n_neighbors+1]\n",
    "print(sort_idx.shape, sort_idx[:,1].shape)\n",
    "\n",
    "rho = [ dist[i, sort_idx[i,0] ] for i in range(n)]\n",
    "rho = np.array(rho)\n",
    "\n",
    "print(rho[0:4])\n",
    "print(rho.shape, rho.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_idx = sort_idx.astype(np.int32)\n",
    "\n",
    "import gc\n",
    "\n",
    "print(gc.collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@numba.jit(nopython=True)\n",
    "def get_weight_function(dists, rho, sigma):\n",
    "    d = dists - rho\n",
    "    #print(d)\n",
    "    d[d<0] = 0\n",
    "    weight = np.exp(- d / sigma )\n",
    "    return weight\n",
    "\n",
    "#@numba.jit(nopython=True)\n",
    "def search_sigma(dists, rho, k, tol = 10**-5, n_iteration=200):\n",
    "    sigma_min = 0\n",
    "    sigma_max = 1000\n",
    "    \n",
    "    cur_sigma = 100\n",
    "    \n",
    "    logk = np.log2(k)\n",
    "    #print(logk)\n",
    "    \n",
    "    for i in range(n_iteration):\n",
    "        \n",
    "        cur_sigma = (sigma_min+sigma_max)/2\n",
    "        probs = get_weight_function(dists,rho,cur_sigma)\n",
    "        weight = np.sum(probs)\n",
    "        #print(weight)\n",
    "        \n",
    "        if np.abs(logk - weight) < tol:\n",
    "            break\n",
    "        \n",
    "        if weight < logk:\n",
    "            sigma_min = cur_sigma\n",
    "        else:\n",
    "            sigma_max = cur_sigma\n",
    "        \n",
    "    return cur_sigma, probs\n",
    "\n",
    "#sigma, weights = search_sigma(dists = dist[0,sort_idx[0,:]],rho = rho[0],k = n_neighbors)\n",
    "\n",
    "#print(np.sum(np.exp( -(dist[0,1:] - rho[0]) / sigma ) ))\n",
    "\n",
    "#print(dist[0,:] - rho[0], dist[0,0], rho[0])\n",
    "\n",
    "#print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#'''\n",
    "def sigma_probs(dist,sort_idx,rho,k):\n",
    "    sigmas = []\n",
    "\n",
    "    directed_graph = []\n",
    "    for i in prange(n):\n",
    "        if (i+1)%1000 == 0:\n",
    "            print('Processed ', i+1, ' of ', n, ' samples.')\n",
    "        sigma, weights = search_sigma(dists = dist[i,sort_idx[i,:]],rho = rho[i],k = k)\n",
    "\n",
    "        probs = np.zeros(n)\n",
    "        probs[sort_idx[i,:]] = weights\n",
    "        #print(sum(weights), np.log2(n_neighbors))\n",
    "        #print(sort_idx[i,:])\n",
    "        #print(probs[1770:1780])\n",
    "\n",
    "        directed_graph.append(probs)\n",
    "\n",
    "    prob = np.array(directed_graph).astype(np.float32)\n",
    "    return prob\n",
    "#'''  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = []\n",
    "\n",
    "directed_graph = []\n",
    "\n",
    "\n",
    "'''\n",
    "for i in range(n):\n",
    "    if (i+1)%1000 == 0:\n",
    "        print('Processed ', i+1, ' of ', n, ' samples.')\n",
    "    sigma, weights = search_sigma(dists = dist[i,sort_idx[i,:]],rho = rho[i],k = n_neighbors)\n",
    "    \n",
    "    probs = np.zeros(n)\n",
    "    probs[sort_idx[i,:]] = weights\n",
    "    #print(sum(weights), np.log2(n_neighbors))\n",
    "    #print(sort_idx[i,:])\n",
    "    #print(probs[1770:1780])\n",
    "    \n",
    "    directed_graph.append(probs)\n",
    "\n",
    "prob = np.array(directed_graph).astype(np.float32)\n",
    "#'''  \n",
    "\n",
    "#with open('probs_n_neighbor5.npy', 'rb') as f: #fmnist_probs_n_neighbor30\n",
    "#    prob = np.load(f)\n",
    "\n",
    "#with open('probs_n_neighbor5.npy', 'wb') as f:\n",
    "#    np.save(f,prob)\n",
    "\n",
    "#print(prob.shape)\n",
    "\n",
    "\n",
    "#print(prob[0:10,0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "import numba \n",
    "from numba import prange\n",
    "\n",
    "#P = prob + np.transpose(prob) - np.multiply(prob, np.transpose(prob))\n",
    "#P = (prob + np.transpose(prob)) / 2\n",
    "\n",
    "with open('fmnist_undirected_graph.npy', 'rb') as f:\n",
    "    P = np.load(f)\n",
    "\n",
    "    \n",
    "'''\n",
    "import numba \n",
    "from numba import prange\n",
    "@numba.jit(nopython=True, parallel=True)\n",
    "def symmetrization_step(prob):\n",
    "    P = np.zeros((n,n),dtype=np.float32)\n",
    "\n",
    "    for i in prange(n):\n",
    "        #if i%1000 == 0:\n",
    "        #    print('Completed ', i, ' of ', n)\n",
    "        for j in prange(i,n):\n",
    "            p = prob[i,j] + prob[j,i] - prob[i,j] * prob[j,i]\n",
    "            P[i,j] = p\n",
    "            P[j,i] = p\n",
    "            \n",
    "    return P\n",
    "\n",
    "\n",
    "P = symmetrization_step(prob)\n",
    "#'''\n",
    "\n",
    "print(np.sum(P[0,:]==0))\n",
    "\n",
    "\n",
    "\n",
    "#with open('fmnist_undirected_graph_n_neighbor5.npy', 'wb') as f:\n",
    "#    np.save(f, P)\n",
    "    \n",
    "#with open('fmnist_undirected_graph_n_neighbor5.npy', 'rb') as f:\n",
    "#    P = np.load(f)\n",
    "\n",
    "print(P.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sort_idx[0:3,0:3])\n",
    "\n",
    "print(P[1,37550], P[37550,1])\n",
    "\n",
    "#print(np.sum(P>1))\n",
    "\n",
    "print(np.sum(P[2,:]==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 3, 300)\n",
    "\n",
    "def f(x, min_dist):\n",
    "    y = []\n",
    "    for i in range(len(x)):\n",
    "        if(x[i] <= min_dist):\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(np.exp(- x[i] + min_dist))\n",
    "    return y\n",
    "\n",
    "dist_low_dim = lambda x, a, b: 1 / (1 + a*x**(2*b))\n",
    "\n",
    "p , _ = optimize.curve_fit(dist_low_dim, x, f(x, MIN_DIST))#MIN_DIST))\n",
    "\n",
    "a = p[0]\n",
    "b = p[1] \n",
    "print(\"Hyperparameters a = \" + str(a) + \" and b = \" + str(b))\n",
    "\n",
    "x_p = np.arange(0,3,0.01)\n",
    "y_p = np.exp(- (x_p-MIN_DIST) * ( (x_p - MIN_DIST) >=0 ) )\n",
    "y_p2 = 1 / (1 + a*x_p**(2*b))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_p,y_p, label='Target')\n",
    "plt.plot(x_p,y_p2, label='Fitted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from network_sig import network\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = network(channels=[X_train.shape[1],2000,3000,2000,1000,2]).to(device)\n",
    "#model.load_state_dict(torch.load('nets_MSE_CE_UMAP_fmnist/epoch3.pth'))\n",
    "\n",
    "criterion_mse = torch.nn.MSELoss()\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "x_init = pca.fit_transform(X_train)\n",
    "\n",
    "\n",
    "Y = []\n",
    "X_train32 = X_train.astype(np.float32)\n",
    "\n",
    "print('Conversion Done')\n",
    "X_torch = torch.as_tensor(X_train32, dtype=torch.float32).to(device)\n",
    "X_torch_test = torch.as_tensor(X_test, dtype=torch.float32).to(device)\n",
    "X_init = torch.as_tensor(x_init, dtype=torch.float32).to(device)\n",
    "print('Tensor Build Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(n):\n",
    "        if i%10000 == 0:\n",
    "            print('completed ', i, ' of ', n)\n",
    "            \n",
    "        x_st = X_torch[i:i+1,:]\n",
    "        #print(x_st.size())\n",
    "        y_st = model(x_st).cpu().numpy().reshape(-1)\n",
    "        #print(y_st.shape)\n",
    "        Y.append(y_st)\n",
    "\n",
    "Y = np.array(Y)\n",
    "print(Y.shape)\n",
    "\n",
    "\n",
    "plt.figure()#figsize=(20,15))\n",
    "plt.scatter(Y[:,0], Y[:,1], c = y_train.astype(int), cmap = 'Spectral', s = 20)\n",
    "plt.title(\"Random Init\")\n",
    "plt.xlabel(\"Axis-1\") #, fontsize = 20); \n",
    "plt.ylabel(\"Axis-2\") #, fontsize = 20)\n",
    "cbar = plt.colorbar(boundaries=np.arange(11)-0.5)\n",
    "cbar.set_ticks(np.arange(10))\n",
    "cbar.set_ticklabels(classes)\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion_pos_old(x,y,a,b):\n",
    "    factor = 1 + a * torch.sum((x-y)**2, dim = 0) ** b\n",
    "    y = torch.log(factor)\n",
    "    \n",
    "    return y\n",
    "\n",
    "def criterion_neg_old(x,y,a,b):\n",
    "    factor = a * torch.sum((x-y)**2) ** b\n",
    "    y = - torch.log(0.0001+factor) + torch.log(1 + factor)\n",
    "    \n",
    "    return y\n",
    "\n",
    "def phi(x,y,a,b):\n",
    "    factor = 1 + a * torch.sum((x-y)**2+10**-8, dim=1) ** b\n",
    "    y = 1/factor\n",
    "    \n",
    "    #for i in range(y.size()[0]):\n",
    "    #    if np.isnan(y[i].cpu().detach().item()):\n",
    "    #        print('Isnan in Pos Crit:', i, x[i], y[i])\n",
    "    \n",
    "    return y\n",
    "\n",
    "def criterion_pos(x,y,p,a,b):\n",
    "    prob = phi(x,y,a,b)\n",
    "    y = -torch.log(prob)*p\n",
    "    #for i in range(y.size()[0]):\n",
    "    #    if np.isnan(y[i].cpu().detach().item()):\n",
    "    #        print('Isnan in Pos Crit:', i)\n",
    "    \n",
    "    return torch.sum(y)\n",
    "\n",
    "def criterion_neg(x,y,p,a,b):\n",
    "    prob = phi(x,y,a,b)\n",
    "    y = -torch.log(1-prob+0.000001)*(1-p)\n",
    "    #for i in range(y.size()[0]):\n",
    "    #    if np.isnan(y[i].cpu().detach().item()):\n",
    "    #        print('Isnan in Neg Crit:', i)\n",
    "    \n",
    "    return torch.sum(y)\n",
    "    \n",
    "\n",
    "Y = model(X_torch[0:10,:])\n",
    "Z = criterion_pos(Y[0:4,:],Y[4:8,:],1.0,1.0,1.0)\n",
    "print(Y.size(), Z.size(), Z.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_train(data_x, data_y, model, criterion, optimizer, epochs, batch_size):\n",
    "    N = data_x.shape[0]\n",
    "    it_per_epoch = int(N/batch_size)\n",
    "    \n",
    "    model = model.train()\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        for j in range(it_per_epoch):\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            choices = np.random.choice(N, batch_size)\n",
    "\n",
    "            X = data_x[choices,:]\n",
    "            Y_est = model(X)\n",
    "            Y = data_y[choices,:]\n",
    "\n",
    "            loss = criterion(Y_est,Y)\n",
    "            losses.append(loss.detach().cpu().item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print('Completed ', i, ' of ', epochs, ' epochs')\n",
    "    \n",
    "    return model, optimizer, losses\n",
    "\n",
    "model, optimizer, losses = pre_train(data_x = X_torch, \n",
    "                                     data_y = X_init, \n",
    "                                     model = model,\n",
    "                                     criterion = criterion_mse,\n",
    "                                     optimizer=optimizer,\n",
    "                                     epochs = 20,\n",
    "                                     batch_size=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.manifold import SpectralEmbedding\n",
    "\n",
    "\n",
    "#LEARNING_RATE = 1\n",
    "epochs = 40\n",
    "\n",
    "#xpp = xpp\n",
    "#print(k)\n",
    "\n",
    "print(n)\n",
    "\n",
    "batch_size = 60\n",
    "it_per_epoch = int(n/batch_size)\n",
    "print('Iteration Per Epoch:', it_per_epoch)\n",
    "\n",
    "CE_array = []\n",
    "results = []\n",
    "print(\"Running Gradient Descent: \\n\")\n",
    "\n",
    "Nan_info = False\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for idx in range(n): #:\n",
    "        choices_0 = np.random.choice(n, batch_size)\n",
    "        k_ch = np.random.randint(low = 0, high = n_neighbors, size=batch_size)\n",
    "        choices_1 = sort_idx[choices_0,k_ch]\n",
    "        \n",
    "        #print(choices_0.shape,choices_1.shape)\n",
    "        \n",
    "        Prob_sample = torch.as_tensor(P[choices_0,choices_1].reshape(-1), dtype=torch.float32).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        Y0 = model(X_torch[choices_0,:])\n",
    "        Y1 = model(X_torch[choices_1,:])\n",
    "        \n",
    "        cr = criterion_pos(Y0, Y1, Prob_sample, a, b)\n",
    "        loss = loss + cr\n",
    "        #print(cr)\n",
    "        \n",
    "        \n",
    "        if np.isnan(cr.detach().cpu().item()):\n",
    "            Nan_info = True\n",
    "            print(epoch,idx,cr)\n",
    "            break\n",
    "        \n",
    "        CE_array.append(cr.cpu().detach().item())\n",
    "        \n",
    "        #gr_1 = get_grad_logphi(y[i,:], y[ij,:],a,b)\n",
    "        #y[i,:] = y[i,:] + LEARNING_RATE * gr_1\n",
    "        \n",
    "        for j in range(5):\n",
    "            k_ch = np.random.randint(low = 0, high = n, size=batch_size)\n",
    "            #Y0 = model(X_torch[i:i+1,:])\n",
    "            Y1 = model(X_torch[k_ch,:])\n",
    "            Prob_sample = torch.as_tensor(P[choices_0,k_ch].reshape(-1), dtype=torch.float32).to(device)\n",
    "            \n",
    "            cr = criterion_neg(Y0, Y1, Prob_sample, a, b)\n",
    "            loss = loss + cr\n",
    "            #print('inside loop: ', cr)\n",
    "            if np.isnan(cr.detach().cpu().item()):\n",
    "                Nan_info = True\n",
    "                print('inside loop: ', epoch, idx, j, cr)\n",
    "                break\n",
    "            \n",
    "        if Nan_info == True:\n",
    "            print('EVERYTHING IS NAN')\n",
    "            break\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "                \n",
    "    #LEARNING_RATE = 1.0 - epoch / epochs\n",
    "    if (epoch+1)%1 == 0:\n",
    "        print('Completed ', epoch , ' of ', epochs)\n",
    "        torch.save(model.state_dict(), 'nets_PCA_INIT_NEW/epoch'+str(epoch)+'.pth')\n",
    "        \n",
    "        Y = []\n",
    "        Y_test = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(n):\n",
    "                x_st = X_torch[i:i+1,:]\n",
    "                y_st = model(x_st).cpu().numpy().reshape(-1)\n",
    "                #print(y_st.shape)\n",
    "                Y.append(y_st)\n",
    "            \n",
    "\n",
    "            \n",
    "            for i in range(X_test.shape[0]):\n",
    "                x_st = X_torch_test[i:i+1,:]\n",
    "                #print(x_st.size())\n",
    "                y_st = model(x_st).cpu().numpy().reshape(-1)\n",
    "                #print(y_st.shape)\n",
    "                Y_test.append(y_st)\n",
    "        \n",
    "        Y_test = np.array(Y_test)\n",
    "        Y = np.array(Y)\n",
    "        \n",
    "        neigh.fit(Y, y_train) \n",
    "        y_nene_out = neigh.predict(Y_test)\n",
    "        \n",
    "        result = 1-np.mean(y_nene_out==y_test)\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        d = {}\n",
    "        d['Y'] = Y\n",
    "        d['Y_test'] = Y_test\n",
    "        savemat('nets_PCA_INIT_NEW/test_data'+str(epoch)+'.mat', d)\n",
    "        \n",
    "        print('Error :', result)\n",
    "        \n",
    "    if (epoch+1)/10 == 0:\n",
    "        lr = lr/10\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(n):\n",
    "        if i%10000 == 0:\n",
    "            print('completed ', i, ' of ', n)\n",
    "            \n",
    "        x_st = X_torch[i:i+1,:]\n",
    "        #print(x_st.size())\n",
    "        y_st = model(x_st).cpu().numpy().reshape(-1)\n",
    "        #print(y_st.shape)\n",
    "        Y.append(y_st)\n",
    "\n",
    "Y = np.array(Y)\n",
    "print(Y)\n",
    "\n",
    "plt.figure()#figsize=(20,15))\n",
    "plt.scatter(Y[:,0], Y[:,1], c = y_train.astype(int), cmap = 'Spectral', s = 0.01)\n",
    "plt.title(\"UMAP\")\n",
    "plt.xlabel(\"UMAP 1\") #, fontsize = 20); \n",
    "plt.ylabel(\"UMAP 2\") #, fontsize = 20)\n",
    "cbar = plt.colorbar(boundaries=np.arange(11)-0.5)\n",
    "cbar.set_ticks(np.arange(10))\n",
    "cbar.set_ticklabels(classes)\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CE_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (epoch+1)/5 == 0:\n",
    "    lr = 0.001/100\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
